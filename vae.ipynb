{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T00:46:37.627763Z",
     "start_time": "2025-01-22T00:46:34.705631Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:44:02.888838Z",
     "start_time": "2025-01-22T02:43:57.584314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_excel(\"湖北省小时数据.xlsx\")  # 替换为你的文件路径\n",
    "\n",
    "# 特征列\n",
    "features = [\"type\", \"weather\", \"wind\", \"humidity\", \"barometer\", \"load\"]\n",
    "data = data[features]\n",
    "\n",
    "# 独热编码 type 列\n",
    "data = pd.get_dummies(data, columns=[\"type\"], prefix=\"type\")\n",
    "\n",
    "# 获取所有列\n",
    "type_columns = [col for col in data.columns if col.startswith(\"type_\")]\n",
    "numeric_columns = [col for col in data.columns if col not in type_columns]\n",
    "\n",
    "# 对非 type 列归一化\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# 保存完整的 type 数据，用于生成 Y\n",
    "complete_type_data = data[type_columns].copy()\n"
   ],
   "id": "126dfc5a36573713",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:45:03.072157Z",
     "start_time": "2025-01-22T02:44:04.920741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建滑动窗口\n",
    "def create_sliding_window(data_X, data_Y, window_size=24, type_columns=None):\n",
    "    \"\"\"\n",
    "    创建滑动窗口数据集\n",
    "    - X 包含完整输入数据\n",
    "    - Y 包含完整的目标数据\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data_X) - window_size):\n",
    "        # 输入：连续 window_size 条数据\n",
    "        X.append(data_X.iloc[i:i+window_size].values)\n",
    "        # 输出：连续 window_size 条数据的完整 type\n",
    "        Y.append(data_Y.iloc[i:i+window_size][type_columns].values)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# 创建输入和目标数据\n",
    "X, Y = create_sliding_window(data, complete_type_data, window_size=24, type_columns=type_columns)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ],
   "id": "42be0b81a9208a71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117104/2991777504.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:45:03.212890Z",
     "start_time": "2025-01-22T02:45:03.076321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 引入缺失值（在训练集和测试集上分别操作）\n",
    "def introduce_missing_types(X, type_columns, missing_fraction=0.1):\n",
    "    \"\"\"\n",
    "    在滑动窗口生成的数据集内随机将一个样本的所有 type 标签置为 0\n",
    "    \"\"\"\n",
    "    X_with_missing = X.clone()  # 避免修改原始数据\n",
    "    num_samples = X_with_missing.shape[0]  # 样本数\n",
    "    num_missing = int(num_samples * missing_fraction)\n",
    "\n",
    "    # 随机选择缺失样本的索引\n",
    "    missing_indices = np.random.choice(num_samples, num_missing, replace=False)\n",
    "\n",
    "    # 将缺失样本的 type 列置为 0\n",
    "    for idx in missing_indices:\n",
    "        X_with_missing[idx, :, :len(type_columns)] = 0  # 假设 type 列在前\n",
    "    return X_with_missing\n",
    "\n",
    "# 在训练集和测试集上引入缺失值\n",
    "X_train_with_missing = introduce_missing_types(X_train, type_columns, missing_fraction=0.1)\n",
    "X_test_with_missing = introduce_missing_types(X_test, type_columns, missing_fraction=0.1)\n",
    "\n",
    "# 输出数据形状\n",
    "print(\"X_train_with_missing shape:\", X_train_with_missing.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)"
   ],
   "id": "b2980b61391d027a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_with_missing shape: torch.Size([63091, 24, 126])\n",
      "Y_train shape: torch.Size([63091, 24, 121])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:57:04.737150Z",
     "start_time": "2025-01-22T02:57:04.730570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConditionalVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional VAE:\n",
    "      - 编码器 (encoder): LSTM, 输入 X => 得到 (mu, logvar)\n",
    "      - 重参数化 (reparameterize): mu + eps*sigma => z\n",
    "      - 解码器 (decoder): 以 (z, X) => 生成 Y( logits )\n",
    "      \n",
    "    适用于: (batch_size, seq_len, x_dim) => (batch_size, seq_len, y_dim)\n",
    "    其中 y_dim = 121 (one-hot 分类)\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, x_dim, y_dim, hidden_dim, latent_dim, num_layers, use_x_in_decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.use_x_in_decoder = use_x_in_decoder\n",
    "\n",
    "        # ---------- Encoder: LSTM -> (h_n) -> mu, logvar ---------- #\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size = x_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # ---------- Decoder: LSTM -> Y(logits) ---------- #\n",
    "        # 如果 decoder 也要用 X 信息，可以在输入里拼接 z + X\n",
    "        if self.use_x_in_decoder:\n",
    "            self.decoder_input_dim = x_dim + latent_dim\n",
    "        else:\n",
    "            self.decoder_input_dim = latent_dim\n",
    "\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size = self.decoder_input_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        # 输出层, 将 hidden_dim -> y_dim(=121), 作为分类 logits\n",
    "        self.output_layer = nn.Linear(hidden_dim, y_dim)\n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch_size, seq_len, x_dim)\n",
    "        返回: mu, logvar => (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "        # LSTM 输出: out, (h_n, c_n)\n",
    "        #   out: (batch_size, seq_len, hidden_dim)\n",
    "        #   h_n: (num_layers, batch_size, hidden_dim)\n",
    "        _, (h_n, _) = self.encoder_lstm(X)\n",
    "        h_last = h_n[-1]  # 取最顶层 hidden state, shape (batch_size, hidden_dim)\n",
    "\n",
    "        mu = self.mu_layer(h_last)\n",
    "        logvar = self.logvar_layer(h_last)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, X=None):\n",
    "        \"\"\"\n",
    "        z: (batch_size, latent_dim)\n",
    "        X: (batch_size, seq_len, x_dim), 如果 use_x_in_decoder=True, 拼到输入\n",
    "        返回: Y_logits: (batch_size, seq_len, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # 初始化 (h_0, c_0)\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=z.device)\n",
    "        c_0 = torch.zeros_like(h_0)\n",
    "\n",
    "        if self.use_x_in_decoder and X is not None:\n",
    "            # 在每个 time step 拼接 [z, X_t]\n",
    "            # 先把 z 扩展到 (batch_size, seq_len, latent_dim)\n",
    "            z_repeated = z.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "            decoder_input = torch.cat([z_repeated, X], dim=-1)  # shape: (batch_size, seq_len, x_dim + latent_dim)\n",
    "        else:\n",
    "            # 只用 z, 不拼 X\n",
    "            z_repeated = z.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "            decoder_input = z_repeated\n",
    "\n",
    "        out, _ = self.decoder_lstm(decoder_input, (h_0, c_0))  # (batch_size, seq_len, hidden_dim)\n",
    "        Y_logits = self.output_layer(out)                      # (batch_size, seq_len, y_dim)\n",
    "        return Y_logits\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向: X => encode => reparameterize => decode => Y_logits\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(X)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        Y_logits = self.decode(z, X if self.use_x_in_decoder else None)\n",
    "        return Y_logits, mu, logvar\n"
   ],
   "id": "1547516c4cba6943",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:57:05.275591Z",
     "start_time": "2025-01-22T02:57:05.272539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cvae_loss_ce(Y_logits, Y_onehot, mu, logvar):\n",
    "    \"\"\"\n",
    "    Y_logits: (batch_size, seq_len, 121)  -- decoder输出的 logits\n",
    "    Y_onehot: (batch_size, seq_len, 121)  -- 真实的 one-hot\n",
    "    mu, logvar: (batch_size, latent_dim)\n",
    "    \n",
    "    返回:\n",
    "        total_loss = CE + KL\n",
    "        ce_loss\n",
    "        kld\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, num_class = Y_logits.shape\n",
    "\n",
    "    # 1) 把 logits reshape\n",
    "    Y_logits_2d = Y_logits.view(-1, num_class)  # (batch_size*seq_len, 121)\n",
    "    # 2) one-hot 转 index\n",
    "    Y_label = Y_onehot.argmax(dim=-1).view(-1)  # (batch_size*seq_len,)\n",
    "\n",
    "    ce_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    ce_loss = ce_fn(Y_logits_2d, Y_label) / batch_size\n",
    "\n",
    "    # KL 散度\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    kld = torch.mean(kld)\n",
    "\n",
    "    total_loss = ce_loss + kld\n",
    "    return total_loss, ce_loss, kld\n"
   ],
   "id": "fc550998168bf618",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:57:05.875492Z",
     "start_time": "2025-01-22T02:57:05.872918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(Y_logits, Y_onehot):\n",
    "    \"\"\"\n",
    "    对 decoder 输出的 logits (batch_size, seq_len, 121)，\n",
    "    与真实 one-hot (batch_size, seq_len, 121) 计算准确率\n",
    "    \"\"\"\n",
    "    pred_label = Y_logits.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "    true_label = Y_onehot.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "    correct = (pred_label == true_label).float().sum()\n",
    "    total = pred_label.numel()  # batch_size * seq_len\n",
    "    acc = correct / total\n",
    "    return acc.item()\n"
   ],
   "id": "65ba13e53dd966c4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:03:15.905748Z",
     "start_time": "2025-01-22T03:03:15.902677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_missing_accuracy(model, data_loader, x_dim, type_dim, device):\n",
    "    \"\"\"\n",
    "    逐批统计 overall_acc、missing_acc 累加，再取平均\n",
    "    返回: (avg_overall_acc, avg_missing_acc)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_overall_acc = 0.0\n",
    "    total_missing_acc = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in data_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            Yb = Yb.to(device)\n",
    "\n",
    "            Y_logits, _, _ = model(Xb)  # (batch_size, seq_len, y_dim)\n",
    "            overall_acc, missing_acc = compute_accuracy_both(Xb, Y_logits, Yb, x_dim, type_dim)\n",
    "\n",
    "            total_overall_acc += overall_acc\n",
    "            total_missing_acc += missing_acc\n",
    "            count += 1\n",
    "\n",
    "    avg_overall_acc = total_overall_acc / count if count > 0 else 0.0\n",
    "    avg_missing_acc = total_missing_acc / count if count > 0 else 0.0\n",
    "\n",
    "    return avg_overall_acc, avg_missing_acc\n"
   ],
   "id": "7ac13e8fc0fbdf24",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:03:26.596503Z",
     "start_time": "2025-01-22T03:03:26.591826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy_both(Xb, Y_logits, Yb, x_dim, type_dim):\n",
    "    \"\"\"\n",
    "    返回:\n",
    "      overall_acc: 整个 batch_size * seq_len 的准确率\n",
    "      missing_acc: 仅在 \"Xb 最后 type_dim 列全部为 0\" 的时间步上的准确率\n",
    "    \"\"\"\n",
    "    # 1) argmax\n",
    "    pred_label = Y_logits.argmax(dim=-1)   # (batch_size, seq_len)\n",
    "    true_label = Yb.argmax(dim=-1)         # (batch_size, seq_len)\n",
    "\n",
    "    # 2) overall\n",
    "    correct_all = (pred_label == true_label).sum().float()\n",
    "    total_all = pred_label.numel()\n",
    "    overall_acc = correct_all / total_all if total_all > 0 else 0.0\n",
    "\n",
    "    # 3) missing_mask: (batch_size, seq_len) = True表示type全部为0\n",
    "    start_type_idx = x_dim - type_dim\n",
    "    missing_mask = (Xb[..., start_type_idx:].sum(dim=-1) == 0.0)\n",
    "\n",
    "    # 展开成 1D\n",
    "    mask_flat = missing_mask.view(-1)\n",
    "    pred_flat = pred_label.view(-1)\n",
    "    true_flat = true_label.view(-1)\n",
    "\n",
    "    pred_missing = pred_flat[mask_flat]\n",
    "    true_missing = true_flat[mask_flat]\n",
    "\n",
    "    correct_missing = (pred_missing == true_missing).sum().float()\n",
    "    total_missing = mask_flat.sum().float()\n",
    "    missing_acc = correct_missing / total_missing if total_missing > 0 else 0.0\n",
    "\n",
    "    return overall_acc, missing_acc\n"
   ],
   "id": "ff333f7313e24d86",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-22T03:04:56.357675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 请确保以下张量已经准备好:\n",
    "#   X_train_with_missing: shape [N, 24, 126], float32\n",
    "#   Y_train: shape [N, 24, 121], float32 (one-hot)\n",
    "#   X_test_with_missing: shape [N_test, 24, 126]\n",
    "#   Y_test: shape [N_test, 24, 121]\n",
    "#\n",
    "# 如果 Y_train / Y_test 里是 one-hot，就可以直接用 cvae_loss_ce()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 超参数\n",
    "seq_len = 24\n",
    "x_dim = 126\n",
    "y_dim = 121   # one-hot 分类，共 121 类\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 200\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_ds = TensorDataset(X_train_with_missing, Y_train)\n",
    "test_ds  = TensorDataset(X_test_with_missing,  Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = ConditionalVAE(\n",
    "    seq_len=seq_len,\n",
    "    x_dim=x_dim,\n",
    "    y_dim=y_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers=num_layers,\n",
    "    use_x_in_decoder=False  # Decoder 也能看到 X\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss_sum = 0.0\n",
    "\n",
    "    for Xb, Yb in train_loader:\n",
    "        Xb = Xb.to(device)  # (batch_size, seq_len, x_dim)\n",
    "        Yb = Yb.to(device)  # (batch_size, seq_len, y_dim=121)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向\n",
    "        Y_logits, mu, logvar = model(Xb)\n",
    "\n",
    "        # 交叉熵 + KL\n",
    "        total_loss, ce_loss, kld_loss = cvae_loss_ce(Y_logits, Yb, mu, logvar)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sum += total_loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "    # 验证(计算准确率)\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            Yb = Yb.to(device)\n",
    "\n",
    "            Y_logits, mu, logvar = model(Xb)\n",
    "            # 计算准确率\n",
    "            acc = compute_accuracy(Y_logits, Yb)\n",
    "            total_acc += acc\n",
    "            count += 1\n",
    "\n",
    "    avg_test_acc = total_acc / count\n",
    "    avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "    # ====== 2. 在整个训练集上计算 overall_acc & missing_acc ======\n",
    "    train_overall_acc, train_missing_acc = evaluate_missing_accuracy(\n",
    "        model, train_loader, x_dim=126, type_dim=121, device=device\n",
    "    )\n",
    "\n",
    "    # ====== 3. 在测试集上计算 overall_acc & missing_acc ======\n",
    "    test_overall_acc, test_missing_acc = evaluate_missing_accuracy(\n",
    "        model, test_loader, x_dim=126, type_dim=121, device=device\n",
    "    )\n",
    "\n",
    "    # ====== 4. 打印结果 ======\n",
    "    print(f\"Epoch {epoch}/{epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Train Overall Acc: {train_overall_acc:.4f} | Train Missing Acc: {train_missing_acc:.4f} | \"\n",
    "          f\"Test Overall Acc: {test_overall_acc:.4f} | Test Missing Acc: {test_missing_acc:.4f}\")\n"
   ],
   "id": "1e57dd2407634b0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 53.8526 | Train Overall Acc: 0.3777 | Train Missing Acc: 0.2613 | Test Overall Acc: 0.3795 | Test Missing Acc: 0.2539\n",
      "Epoch 2/200 | Train Loss: 45.6270 | Train Overall Acc: 0.4350 | Train Missing Acc: 0.2650 | Test Overall Acc: 0.4362 | Test Missing Acc: 0.2562\n",
      "Epoch 3/200 | Train Loss: 42.4480 | Train Overall Acc: 0.5054 | Train Missing Acc: 0.2576 | Test Overall Acc: 0.5067 | Test Missing Acc: 0.2493\n",
      "Epoch 4/200 | Train Loss: 39.1940 | Train Overall Acc: 0.5656 | Train Missing Acc: 0.2695 | Test Overall Acc: 0.5659 | Test Missing Acc: 0.2602\n",
      "Epoch 5/200 | Train Loss: 37.4035 | Train Overall Acc: 0.5904 | Train Missing Acc: 0.2677 | Test Overall Acc: 0.5918 | Test Missing Acc: 0.2572\n",
      "Epoch 6/200 | Train Loss: 36.3787 | Train Overall Acc: 0.6035 | Train Missing Acc: 0.2724 | Test Overall Acc: 0.6040 | Test Missing Acc: 0.2671\n",
      "Epoch 7/200 | Train Loss: 35.8486 | Train Overall Acc: 0.6119 | Train Missing Acc: 0.2580 | Test Overall Acc: 0.6130 | Test Missing Acc: 0.2531\n",
      "Epoch 8/200 | Train Loss: 35.5218 | Train Overall Acc: 0.6125 | Train Missing Acc: 0.2657 | Test Overall Acc: 0.6124 | Test Missing Acc: 0.2580\n",
      "Epoch 9/200 | Train Loss: 35.2126 | Train Overall Acc: 0.6218 | Train Missing Acc: 0.2667 | Test Overall Acc: 0.6231 | Test Missing Acc: 0.2641\n",
      "Epoch 10/200 | Train Loss: 34.9275 | Train Overall Acc: 0.6256 | Train Missing Acc: 0.2742 | Test Overall Acc: 0.6264 | Test Missing Acc: 0.2674\n",
      "Epoch 11/200 | Train Loss: 34.5860 | Train Overall Acc: 0.6300 | Train Missing Acc: 0.2725 | Test Overall Acc: 0.6313 | Test Missing Acc: 0.2664\n",
      "Epoch 12/200 | Train Loss: 34.3634 | Train Overall Acc: 0.6341 | Train Missing Acc: 0.2751 | Test Overall Acc: 0.6351 | Test Missing Acc: 0.2676\n",
      "Epoch 13/200 | Train Loss: 34.1640 | Train Overall Acc: 0.6337 | Train Missing Acc: 0.2751 | Test Overall Acc: 0.6332 | Test Missing Acc: 0.2687\n",
      "Epoch 14/200 | Train Loss: 33.9545 | Train Overall Acc: 0.6388 | Train Missing Acc: 0.2786 | Test Overall Acc: 0.6390 | Test Missing Acc: 0.2663\n",
      "Epoch 15/200 | Train Loss: 33.8028 | Train Overall Acc: 0.6423 | Train Missing Acc: 0.2775 | Test Overall Acc: 0.6423 | Test Missing Acc: 0.2697\n",
      "Epoch 16/200 | Train Loss: 33.7194 | Train Overall Acc: 0.6401 | Train Missing Acc: 0.2739 | Test Overall Acc: 0.6404 | Test Missing Acc: 0.2697\n",
      "Epoch 17/200 | Train Loss: 33.6286 | Train Overall Acc: 0.6452 | Train Missing Acc: 0.2797 | Test Overall Acc: 0.6442 | Test Missing Acc: 0.2693\n",
      "Epoch 18/200 | Train Loss: 33.4864 | Train Overall Acc: 0.6477 | Train Missing Acc: 0.2755 | Test Overall Acc: 0.6480 | Test Missing Acc: 0.2673\n",
      "Epoch 19/200 | Train Loss: 33.4283 | Train Overall Acc: 0.6495 | Train Missing Acc: 0.2750 | Test Overall Acc: 0.6491 | Test Missing Acc: 0.2688\n",
      "Epoch 20/200 | Train Loss: 33.2935 | Train Overall Acc: 0.6480 | Train Missing Acc: 0.2758 | Test Overall Acc: 0.6462 | Test Missing Acc: 0.2684\n",
      "Epoch 21/200 | Train Loss: 33.1976 | Train Overall Acc: 0.6500 | Train Missing Acc: 0.2758 | Test Overall Acc: 0.6492 | Test Missing Acc: 0.2721\n",
      "Epoch 22/200 | Train Loss: 33.1268 | Train Overall Acc: 0.6515 | Train Missing Acc: 0.2748 | Test Overall Acc: 0.6500 | Test Missing Acc: 0.2682\n",
      "Epoch 23/200 | Train Loss: 33.0053 | Train Overall Acc: 0.6500 | Train Missing Acc: 0.2796 | Test Overall Acc: 0.6496 | Test Missing Acc: 0.2714\n",
      "Epoch 24/200 | Train Loss: 33.0704 | Train Overall Acc: 0.6526 | Train Missing Acc: 0.2784 | Test Overall Acc: 0.6517 | Test Missing Acc: 0.2667\n",
      "Epoch 25/200 | Train Loss: 33.0171 | Train Overall Acc: 0.6553 | Train Missing Acc: 0.2784 | Test Overall Acc: 0.6537 | Test Missing Acc: 0.2699\n",
      "Epoch 26/200 | Train Loss: 32.8159 | Train Overall Acc: 0.6560 | Train Missing Acc: 0.2778 | Test Overall Acc: 0.6550 | Test Missing Acc: 0.2710\n",
      "Epoch 27/200 | Train Loss: 32.7660 | Train Overall Acc: 0.6581 | Train Missing Acc: 0.2786 | Test Overall Acc: 0.6573 | Test Missing Acc: 0.2713\n",
      "Epoch 28/200 | Train Loss: 32.7347 | Train Overall Acc: 0.6584 | Train Missing Acc: 0.2781 | Test Overall Acc: 0.6561 | Test Missing Acc: 0.2675\n",
      "Epoch 29/200 | Train Loss: 32.6120 | Train Overall Acc: 0.6630 | Train Missing Acc: 0.2765 | Test Overall Acc: 0.6612 | Test Missing Acc: 0.2694\n",
      "Epoch 30/200 | Train Loss: 32.5684 | Train Overall Acc: 0.6618 | Train Missing Acc: 0.2766 | Test Overall Acc: 0.6593 | Test Missing Acc: 0.2703\n",
      "Epoch 31/200 | Train Loss: 32.4976 | Train Overall Acc: 0.6610 | Train Missing Acc: 0.2806 | Test Overall Acc: 0.6591 | Test Missing Acc: 0.2700\n",
      "Epoch 32/200 | Train Loss: 32.4747 | Train Overall Acc: 0.6615 | Train Missing Acc: 0.2756 | Test Overall Acc: 0.6589 | Test Missing Acc: 0.2676\n",
      "Epoch 33/200 | Train Loss: 32.3614 | Train Overall Acc: 0.6608 | Train Missing Acc: 0.2790 | Test Overall Acc: 0.6578 | Test Missing Acc: 0.2665\n",
      "Epoch 34/200 | Train Loss: 32.3110 | Train Overall Acc: 0.6638 | Train Missing Acc: 0.2767 | Test Overall Acc: 0.6601 | Test Missing Acc: 0.2660\n",
      "Epoch 35/200 | Train Loss: 32.2875 | Train Overall Acc: 0.6671 | Train Missing Acc: 0.2800 | Test Overall Acc: 0.6641 | Test Missing Acc: 0.2701\n",
      "Epoch 36/200 | Train Loss: 32.2413 | Train Overall Acc: 0.6628 | Train Missing Acc: 0.2768 | Test Overall Acc: 0.6597 | Test Missing Acc: 0.2692\n",
      "Epoch 37/200 | Train Loss: 32.1937 | Train Overall Acc: 0.6661 | Train Missing Acc: 0.2803 | Test Overall Acc: 0.6629 | Test Missing Acc: 0.2704\n",
      "Epoch 38/200 | Train Loss: 32.1189 | Train Overall Acc: 0.6667 | Train Missing Acc: 0.2794 | Test Overall Acc: 0.6633 | Test Missing Acc: 0.2719\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d98bbb2c1e602a6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
