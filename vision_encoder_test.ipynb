{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T03:10:44.661776Z",
     "start_time": "2025-01-22T03:10:43.443270Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:10:48.639383Z",
     "start_time": "2025-01-22T03:10:44.665155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_excel(\"湖北省小时数据.xlsx\")  # 替换为你的文件路径\n",
    "\n",
    "# 特征列\n",
    "features = [\"type\", \"weather\", \"wind\", \"humidity\", \"barometer\", \"load\"]\n",
    "data = data[features]\n",
    "\n",
    "# 独热编码 type 列\n",
    "data = pd.get_dummies(data, columns=[\"type\"], prefix=\"type\")\n",
    "\n",
    "# 获取所有列\n",
    "type_columns = [col for col in data.columns if col.startswith(\"type_\")]\n",
    "numeric_columns = [col for col in data.columns if col not in type_columns]\n",
    "\n",
    "# 对非 type 列归一化\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# 保存完整的 type 数据，用于生成 Y\n",
    "complete_type_data = data[type_columns].copy()\n"
   ],
   "id": "126dfc5a36573713",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:11:48.560587Z",
     "start_time": "2025-01-22T03:10:48.699063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建滑动窗口\n",
    "def create_sliding_window(data_X, data_Y, window_size=24, type_columns=None):\n",
    "    \"\"\"\n",
    "    创建滑动窗口数据集\n",
    "    - X 包含完整输入数据\n",
    "    - Y 包含完整的目标数据\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data_X) - window_size):\n",
    "        # 输入：连续 window_size 条数据\n",
    "        X.append(data_X.iloc[i:i+window_size].values)\n",
    "        # 输出：连续 window_size 条数据的完整 type\n",
    "        Y.append(data_Y.iloc[i:i+window_size][type_columns].values)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# 创建输入和目标数据\n",
    "X, Y = create_sliding_window(data, complete_type_data, window_size=24, type_columns=type_columns)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ],
   "id": "42be0b81a9208a71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127715/2991777504.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:11:48.751620Z",
     "start_time": "2025-01-22T03:11:48.578582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 引入缺失值（在训练集和测试集上分别操作）\n",
    "def introduce_missing_types(X, type_columns, missing_fraction=0.1):\n",
    "    \"\"\"\n",
    "    在滑动窗口生成的数据集内随机将一个样本的所有 type 标签置为 0\n",
    "    \"\"\"\n",
    "    X_with_missing = X.clone()  # 避免修改原始数据\n",
    "    num_samples = X_with_missing.shape[0]  # 样本数\n",
    "    num_missing = int(num_samples * missing_fraction)\n",
    "\n",
    "    # 随机选择缺失样本的索引\n",
    "    missing_indices = np.random.choice(num_samples, num_missing, replace=False)\n",
    "\n",
    "    # 将缺失样本的 type 列置为 0\n",
    "    for idx in missing_indices:\n",
    "        X_with_missing[idx, :, :len(type_columns)] = 0  # 假设 type 列在前\n",
    "    return X_with_missing\n",
    "\n",
    "# 在训练集和测试集上引入缺失值\n",
    "X_train_with_missing = introduce_missing_types(X_train, type_columns, missing_fraction=0.1)\n",
    "X_test_with_missing = introduce_missing_types(X_test, type_columns, missing_fraction=0.1)\n",
    "\n",
    "# 输出数据形状\n",
    "print(\"X_train_with_missing shape:\", X_train_with_missing.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)"
   ],
   "id": "b2980b61391d027a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_with_missing shape: torch.Size([63091, 24, 126])\n",
      "Y_train shape: torch.Size([63091, 24, 121])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:11:48.797774Z",
     "start_time": "2025-01-22T03:11:48.789076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    标准的 Transformer 位置编码实现\n",
    "    参考自 PyTorch 官方教程：\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 偶数维: sin, 奇数维: cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数索引\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数索引\n",
    "\n",
    "        # 注册成缓冲区，不会作为参数训练\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # (batch_size, seq_len, d_model) + (seq_len, d_model)\n",
    "        # 需要broadcast: 因此先在 batch 维度上 unsqueeze(0) 再加\n",
    "        x = x + self.pe[:seq_len].unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "class ConditionalVAE_Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    将原先 LSTM 改为 Transformer 的 Conditional VAE 实现。\n",
    "    形状约定：\n",
    "      X: (batch_size, seq_len, x_dim)\n",
    "      Y: (batch_size, seq_len, y_dim)  (one-hot)\n",
    "\n",
    "    * use_x_in_decoder = True 表示解码器输入 (z, X_t) 拼接后，再通过 Transformer Decoder。\n",
    "      这里简化为使用 TransformerEncoder 来处理整个序列（并行），不做自回归 mask。\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_len,\n",
    "                 x_dim,\n",
    "                 y_dim,\n",
    "                 hidden_dim=128,\n",
    "                 latent_dim=64,\n",
    "                 num_layers=2,\n",
    "                 use_x_in_decoder=False,\n",
    "                 nhead=4,              # transformer多头注意力的头数\n",
    "                 dim_feedforward=256,  # feedforward隐藏层维度\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.use_x_in_decoder = use_x_in_decoder\n",
    "\n",
    "        # ========== Encoder部分 ==========\n",
    "        # 1) 将输入 x_dim 投影到 embed_dim\n",
    "        self.embed_dim = hidden_dim   # 这里可自行设定，比如和 hidden_dim 一样\n",
    "        self.input_proj = nn.Linear(x_dim, self.embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim)\n",
    "\n",
    "        # 2) TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # 若你的PyTorch版本>=1.10,可用batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 3) 从最终的序列表示里取一个向量 (比如最后时刻) => mu, logvar\n",
    "        self.mu_layer = nn.Linear(self.embed_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(self.embed_dim, latent_dim)\n",
    "\n",
    "        # ========== Decoder部分 ==========\n",
    "        # 解码器的输入维度 (z + X) 或单独 z\n",
    "        if self.use_x_in_decoder:\n",
    "            self.decoder_input_dim = x_dim + latent_dim\n",
    "        else:\n",
    "            self.decoder_input_dim = latent_dim\n",
    "\n",
    "        self.decoder_proj = nn.Linear(self.decoder_input_dim, self.embed_dim)\n",
    "        self.pos_decoder = PositionalEncoding(self.embed_dim)\n",
    "\n",
    "        # 这里复用 TransformerEncoder 做并行解码（非自回归）\n",
    "        # 如果要做自回归，可改用 nn.TransformerDecoder\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerEncoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 输出到 y_dim（比如 121 类 one-hot）\n",
    "        self.output_layer = nn.Linear(self.embed_dim, y_dim)\n",
    "\n",
    "    def encode(self, X):\n",
    "        \"\"\"\n",
    "        编码器：X -> (batch, seq_len, embed_dim) -> TransformerEncoder -> 取最后时刻输出 => mu, logvar\n",
    "        \"\"\"\n",
    "        # 1) 投影\n",
    "        X_embed = self.input_proj(X)  # (B, T, embed_dim)\n",
    "        # 2) 加位置编码\n",
    "        X_embed = self.pos_encoder(X_embed)\n",
    "        # 3) TransformerEncoder\n",
    "        #    若 batch_first=True, 输出形状依然 (B, T, embed_dim)\n",
    "        enc_out = self.transformer_encoder(X_embed)  # (B, T, embed_dim)\n",
    "\n",
    "        # 取最后一个 time step 的 hidden state: enc_out[:, -1, :]\n",
    "        # 也可以选择平均池化或别的方式\n",
    "        h_last = enc_out[:, -1, :]  # (B, embed_dim)\n",
    "\n",
    "        mu = self.mu_layer(h_last)\n",
    "        logvar = self.logvar_layer(h_last)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, X=None):\n",
    "        \"\"\"\n",
    "        解码器：z(可以拼接X) -> 投影 -> 位置编码 -> TransformerEncoder -> 输出层\n",
    "        最终得到 Y_logits: (batch_size, seq_len, y_dim)\n",
    "        \"\"\"\n",
    "        B = z.size(0)\n",
    "\n",
    "        # 1) 先把 z 在时间维度上重复\n",
    "        #    z shape: (B, latent_dim) => (B, seq_len, latent_dim)\n",
    "        z_repeated = z.unsqueeze(1).repeat(1, self.seq_len, 1)  # (B, T, latent_dim)\n",
    "\n",
    "        if self.use_x_in_decoder and X is not None:\n",
    "            # 拼接 z 和 X: (B, T, x_dim + latent_dim)\n",
    "            decoder_input = torch.cat([X, z_repeated], dim=-1)\n",
    "        else:\n",
    "            # 只用 z\n",
    "            decoder_input = z_repeated\n",
    "\n",
    "        # 2) 投影到 embed_dim\n",
    "        dec_embed = self.decoder_proj(decoder_input)  # (B, T, embed_dim)\n",
    "        # 3) 位置编码\n",
    "        dec_embed = self.pos_decoder(dec_embed)\n",
    "        # 4) 通过 TransformerEncoder 做并行解码\n",
    "        dec_out = self.transformer_decoder(dec_embed)  # (B, T, embed_dim)\n",
    "        # 5) 输出层 => (B, T, y_dim)\n",
    "        Y_logits = self.output_layer(dec_out)\n",
    "        return Y_logits\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向过程： X -> encode -> reparam -> decode\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(X)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        Y_logits = self.decode(z, X if self.use_x_in_decoder else None)\n",
    "        return Y_logits, mu, logvar\n"
   ],
   "id": "1547516c4cba6943",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:14:20.952244Z",
     "start_time": "2025-01-22T03:14:20.949241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cvae_loss_ce(Y_logits, Y_onehot, mu, logvar):\n",
    "    \"\"\"\n",
    "    Y_logits: (batch_size, seq_len, 121)  -- decoder输出的 logits\n",
    "    Y_onehot: (batch_size, seq_len, 121)  -- 真实的 one-hot\n",
    "    mu, logvar: (batch_size, latent_dim)\n",
    "    \n",
    "    返回:\n",
    "        total_loss = CE + KL\n",
    "        ce_loss\n",
    "        kld\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, num_class = Y_logits.shape\n",
    "\n",
    "    # 1) 把 logits reshape\n",
    "    Y_logits_2d = Y_logits.view(-1, num_class)  # (batch_size*seq_len, 121)\n",
    "    # 2) one-hot 转 index\n",
    "    Y_label = Y_onehot.argmax(dim=-1).view(-1)  # (batch_size*seq_len,)\n",
    "\n",
    "    ce_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    ce_loss = ce_fn(Y_logits_2d, Y_label) / batch_size\n",
    "\n",
    "    # KL 散度\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    kld = torch.mean(kld)\n",
    "\n",
    "    total_loss = ce_loss + kld\n",
    "    return total_loss, ce_loss, kld\n"
   ],
   "id": "fc550998168bf618",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:14:21.613050Z",
     "start_time": "2025-01-22T03:14:21.610250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(Y_logits, Y_onehot):\n",
    "    \"\"\"\n",
    "    对 decoder 输出的 logits (batch_size, seq_len, 121)，\n",
    "    与真实 one-hot (batch_size, seq_len, 121) 计算准确率\n",
    "    \"\"\"\n",
    "    pred_label = Y_logits.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "    true_label = Y_onehot.argmax(dim=-1)  # (batch_size, seq_len)\n",
    "\n",
    "    correct = (pred_label == true_label).float().sum()\n",
    "    total = pred_label.numel()  # batch_size * seq_len\n",
    "    acc = correct / total\n",
    "    return acc.item()\n"
   ],
   "id": "65ba13e53dd966c4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:14:22.062067Z",
     "start_time": "2025-01-22T03:14:22.058797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_missing_accuracy(model, data_loader, x_dim, type_dim, device):\n",
    "    \"\"\"\n",
    "    逐批统计 overall_acc、missing_acc 累加，再取平均\n",
    "    返回: (avg_overall_acc, avg_missing_acc)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_overall_acc = 0.0\n",
    "    total_missing_acc = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in data_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            Yb = Yb.to(device)\n",
    "\n",
    "            Y_logits, _, _ = model(Xb)  # (batch_size, seq_len, y_dim)\n",
    "            overall_acc, missing_acc = compute_accuracy_both(Xb, Y_logits, Yb, x_dim, type_dim)\n",
    "\n",
    "            total_overall_acc += overall_acc\n",
    "            total_missing_acc += missing_acc\n",
    "            count += 1\n",
    "\n",
    "    avg_overall_acc = total_overall_acc / count if count > 0 else 0.0\n",
    "    avg_missing_acc = total_missing_acc / count if count > 0 else 0.0\n",
    "\n",
    "    return avg_overall_acc, avg_missing_acc\n"
   ],
   "id": "7ac13e8fc0fbdf24",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T03:14:22.980605Z",
     "start_time": "2025-01-22T03:14:22.977264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy_both(Xb, Y_logits, Yb, x_dim, type_dim):\n",
    "    \"\"\"\n",
    "    返回:\n",
    "      overall_acc: 整个 batch_size * seq_len 的准确率\n",
    "      missing_acc: 仅在 \"Xb 最后 type_dim 列全部为 0\" 的时间步上的准确率\n",
    "    \"\"\"\n",
    "    # 1) argmax\n",
    "    pred_label = Y_logits.argmax(dim=-1)   # (batch_size, seq_len)\n",
    "    true_label = Yb.argmax(dim=-1)         # (batch_size, seq_len)\n",
    "\n",
    "    # 2) overall\n",
    "    correct_all = (pred_label == true_label).sum().float()\n",
    "    total_all = pred_label.numel()\n",
    "    overall_acc = correct_all / total_all if total_all > 0 else 0.0\n",
    "\n",
    "    # 3) missing_mask: (batch_size, seq_len) = True表示type全部为0\n",
    "    start_type_idx = x_dim - type_dim\n",
    "    missing_mask = (Xb[..., start_type_idx:].sum(dim=-1) == 0.0)\n",
    "\n",
    "    # 展开成 1D\n",
    "    mask_flat = missing_mask.view(-1)\n",
    "    pred_flat = pred_label.view(-1)\n",
    "    true_flat = true_label.view(-1)\n",
    "\n",
    "    pred_missing = pred_flat[mask_flat]\n",
    "    true_missing = true_flat[mask_flat]\n",
    "\n",
    "    correct_missing = (pred_missing == true_missing).sum().float()\n",
    "    total_missing = mask_flat.sum().float()\n",
    "    missing_acc = correct_missing / total_missing if total_missing > 0 else 0.0\n",
    "\n",
    "    return overall_acc, missing_acc\n"
   ],
   "id": "ff333f7313e24d86",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-22T03:14:24.646896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 请确保以下张量已经准备好:\n",
    "#   X_train_with_missing: shape [N, 24, 126], float32\n",
    "#   Y_train: shape [N, 24, 121], float32 (one-hot)\n",
    "#   X_test_with_missing: shape [N_test, 24, 126]\n",
    "#   Y_test: shape [N_test, 24, 121]\n",
    "#\n",
    "# 如果 Y_train / Y_test 里是 one-hot，就可以直接用 cvae_loss_ce()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 超参数\n",
    "seq_len = 24\n",
    "x_dim = 126\n",
    "y_dim = 121   # one-hot 分类，共 121 类\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 200\n",
    "\n",
    "# 额外的 Transformer 参数\n",
    "nhead = 4\n",
    "dim_feedforward = 256\n",
    "dropout = 0.1\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_ds = TensorDataset(X_train_with_missing, Y_train)\n",
    "test_ds  = TensorDataset(X_test_with_missing,  Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = ConditionalVAE_Transformer(\n",
    "    seq_len=seq_len,\n",
    "    x_dim=x_dim,\n",
    "    y_dim=y_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers=num_layers,\n",
    "    use_x_in_decoder=False,  # 是否在Decoder拼接X\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss_sum = 0.0\n",
    "\n",
    "    for Xb, Yb in train_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        Yb = Yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Y_logits, mu, logvar = model(Xb)\n",
    "        total_loss, ce_loss, kld_loss = cvae_loss_ce(Y_logits, Yb, mu, logvar)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_sum += total_loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "    # 验证(计算准确率)\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            Yb = Yb.to(device)\n",
    "\n",
    "            Y_logits, mu, logvar = model(Xb)\n",
    "            # 计算准确率\n",
    "            acc = compute_accuracy(Y_logits, Yb)\n",
    "            total_acc += acc\n",
    "            count += 1\n",
    "\n",
    "    avg_test_acc = total_acc / count\n",
    "    avg_train_loss = total_loss_sum / len(train_loader)\n",
    "\n",
    "    # ====== 2. 在整个训练集上计算 overall_acc & missing_acc ======\n",
    "    train_overall_acc, train_missing_acc = evaluate_missing_accuracy(\n",
    "        model, train_loader, x_dim=126, type_dim=121, device=device\n",
    "    )\n",
    "\n",
    "    # ====== 3. 在测试集上计算 overall_acc & missing_acc ======\n",
    "    test_overall_acc, test_missing_acc = evaluate_missing_accuracy(\n",
    "        model, test_loader, x_dim=126, type_dim=121, device=device\n",
    "    )\n",
    "\n",
    "    # ====== 4. 打印结果 ======\n",
    "    print(f\"Epoch {epoch}/{epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Train Overall Acc: {train_overall_acc:.4f} | Train Missing Acc: {train_missing_acc:.4f} | \"\n",
    "          f\"Test Overall Acc: {test_overall_acc:.4f} | Test Missing Acc: {test_missing_acc:.4f}\")\n"
   ],
   "id": "1e57dd2407634b0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 49.1291 | Train Overall Acc: 0.4188 | Train Missing Acc: 0.2727 | Test Overall Acc: 0.4195 | Test Missing Acc: 0.2739\n",
      "Epoch 2/200 | Train Loss: 44.0132 | Train Overall Acc: 0.4179 | Train Missing Acc: 0.2608 | Test Overall Acc: 0.4165 | Test Missing Acc: 0.2631\n",
      "Epoch 3/200 | Train Loss: 43.5449 | Train Overall Acc: 0.4222 | Train Missing Acc: 0.2060 | Test Overall Acc: 0.4217 | Test Missing Acc: 0.2157\n",
      "Epoch 4/200 | Train Loss: 43.2408 | Train Overall Acc: 0.4537 | Train Missing Acc: 0.2608 | Test Overall Acc: 0.4531 | Test Missing Acc: 0.2653\n",
      "Epoch 5/200 | Train Loss: 41.7163 | Train Overall Acc: 0.5224 | Train Missing Acc: 0.2739 | Test Overall Acc: 0.5231 | Test Missing Acc: 0.2785\n",
      "Epoch 6/200 | Train Loss: 39.8774 | Train Overall Acc: 0.5417 | Train Missing Acc: 0.2764 | Test Overall Acc: 0.5422 | Test Missing Acc: 0.2777\n",
      "Epoch 7/200 | Train Loss: 39.2262 | Train Overall Acc: 0.5503 | Train Missing Acc: 0.2749 | Test Overall Acc: 0.5506 | Test Missing Acc: 0.2804\n",
      "Epoch 8/200 | Train Loss: 38.9067 | Train Overall Acc: 0.5582 | Train Missing Acc: 0.2781 | Test Overall Acc: 0.5586 | Test Missing Acc: 0.2807\n",
      "Epoch 9/200 | Train Loss: 38.8821 | Train Overall Acc: 0.5480 | Train Missing Acc: 0.2759 | Test Overall Acc: 0.5484 | Test Missing Acc: 0.2803\n",
      "Epoch 10/200 | Train Loss: 38.7211 | Train Overall Acc: 0.5578 | Train Missing Acc: 0.2791 | Test Overall Acc: 0.5583 | Test Missing Acc: 0.2780\n",
      "Epoch 11/200 | Train Loss: 38.5618 | Train Overall Acc: 0.5620 | Train Missing Acc: 0.2723 | Test Overall Acc: 0.5626 | Test Missing Acc: 0.2746\n",
      "Epoch 12/200 | Train Loss: 38.4782 | Train Overall Acc: 0.5580 | Train Missing Acc: 0.2777 | Test Overall Acc: 0.5577 | Test Missing Acc: 0.2826\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d98bbb2c1e602a6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
